# LLM Chatbot with RAG & RLHF

A full-stack AI chatbot application that combines **Retrieval-Augmented Generation (RAG)** for real-time knowledge access and a complete **Reinforcement Learning from Human Feedback (RLHF)** pipeline for continuous model improvement.

## ğŸš€ Features

### 1. Interactive Chat & RAG

- **Chat Interface**: React-based UI for interacting with the LLM.
- **Knowledge Ingestion**: Users can upload/ingest text snippets directly from the UI into a Vector Database (ChromaDB).
- **Context Retrieval**: The backend retrieves relevant context for every query to ground the LLM's responses.

### 2. Data Collection & Feedback

- **Conversation History**: All user queries and model responses are stored in PostgreSQL.
- **User Feedback**: Users can rate responses (ğŸ‘ / ğŸ‘), creating a labeled dataset for training.

### 3. Training Pipeline (RLHF)

The application includes three standalone scripts to fine-tune the model based on collected data:

- **Supervised Fine-Tuning (SFT)**: Fine-tunes the base model on conversations with positive ratings.
- **Reward Modeling**: Trains a classifier (The "Judge") to predict human preferences based on feedback history.
- **PPO (Proximal Policy Optimization)**: Uses Reinforcement Learning to align the model's policy using the trained Reward Model.

---

## ğŸ“‚ Project Structure

```text
llm-chatbot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ config.py           # Configuration (LLM paths, DB URLs)
â”‚   â”‚   â”œâ”€â”€ db.py               # Database session management
â”‚   â”‚   â”œâ”€â”€ judge.py            # Reward Model (Judge) logic
â”‚   â”‚   â”œâ”€â”€ llm.py              # LLM generation logic
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI app entrypoint
â”‚   â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models (Conversation, Feedback)
â”‚   â”‚   â”œâ”€â”€ rag.py              # ChromaDB vector store & retrieval logic
â”‚   â”‚   â”œâ”€â”€ routes_chat.py      # FastAPI routes for chat & ingestion
â”‚   â”‚   â”œâ”€â”€ routes_feedback.py  # FastAPI routes for feedback collection
â”‚   â”‚   â”œâ”€â”€ schemas.py          # Pydantic models
â”‚   â”‚   â”œâ”€â”€ train_reward.py     # Script: Train Reward Model
â”‚   â”‚   â”œâ”€â”€ train_rl.py         # Script: Train PPO (RL)
â”‚   â”‚   â””â”€â”€ train_sft.py        # Script: Train SFT
â”‚   |
â”‚   â”œâ”€â”€ db/                     # ChromaDB persistent storage
â”‚   â”‚   â””â”€â”€ chroma/             # ChromaDB database files
|   |       â””â”€â”€ chroma.sqlite3  # ChromaDB database file
|   |
|   â””â”€â”€ requirements.txt        # Python dependencies
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ node_modules/           # Node.js dependencies
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/             # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ Feedback/           # Feedback component
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Feedback.tsx    # Feedback component
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Feedback.module.css       # Feedback component styles
â”‚   â”‚   â”‚   â””â”€â”€ MessageInput/                 # Message input component
â”‚   â”‚   â”‚   â”‚    â””â”€â”€ MessageInput.tsx         # Message input component
â”‚   â”‚   â”‚   â”‚    â””â”€â”€ MessageInput.module.css  # Message input component styles
â”‚   â”‚   â”‚   â””â”€â”€ ReadmeRenderer.tsx            # Renders this README.md file
â”‚   â”‚   â”‚   
â”‚   â”‚   â”œâ”€â”€ api.ts              # Axios API client
â”‚   â”‚   â”œâ”€â”€ App.tsx             # Main App Component
â”‚   â”‚   â”œâ”€â”€ Chat.tsx            # Main Chat Component
â”‚   â”‚   â”œâ”€â”€ global.d.ts         # TypeScript global declarations
â”‚   â”‚   â””â”€â”€ main.tsx            # Main entrypoint
â”‚   â”‚   
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â””â”€â”€ index.css           # Global stylesheet
â”‚   |
â”‚   â”œâ”€â”€ .env                    # Environment variables (Vite)
â”‚   â”œâ”€â”€ .env.example            # Example environment variables file
â”‚   â”œâ”€â”€ index.html              # HTML entrypoint
â”‚   â”œâ”€â”€ node_modules/           # Node.js dependencies
â”‚   â”œâ”€â”€ package.json            # Node.js package config
â”‚   â”œâ”€â”€ tsconfig.json           # TypeScript config
â”‚   â”œâ”€â”€ package-lock.json       # Node.js package lock file
â”‚   â”œâ”€â”€ tsconfig.node.json      # TypeScript config for Node.js
â”‚   â””â”€â”€ vite.config.ts          # Vite.js config
|
â”œâ”€â”€ .env                        # Environment variables (LLM paths, DB URLs)
â”œâ”€â”€ .env.example                # Example environment variables file
â”œâ”€â”€ .gitignore                  # Git ignore file
â”œâ”€â”€ python-version              # Python version file
â”œâ”€â”€ LICENSE                     # License file
â”œâ”€â”€ PostgreSQL.md               # Database setup commands
â”œâ”€â”€ pyproject.toml              # Python project configuration
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ uv.lock                     # uv lock file
```

---

## ğŸ› ï¸ Setup & Installation

### 1. Database Setup

Ensure PostgreSQL is running. Create the database and user using the commands found in `PostgreSQL.md`:

```sql
CREATE DATABASE llm_chatbot_db;
-- See PostgreSQL.md for full permissions setup
```

### 2. Backend

Navigate to the backend directory and install dependencies (assuming `requirements.txt` exists):

```bash
cd backend
uv pip install fastapi uvicorn sqlalchemy psycopg2-binary transformers trl torch langchain-chroma langchain-huggingface

# Run the server
uvicorn app.main:app --reload
```

### 3. Frontend

Navigate to the frontend directory:

```bash
cd frontend
npm install
npm run dev # for development
npm start # for production
```

---

## ğŸ”„ Workflows

### A. The RAG Workflow (Runtime)

1. **Ingest**: User pastes text into the UI and clicks **"Ingest (RAG)"**. The text is embedded and stored in ChromaDB.
2. **Chat**: User asks a question.
3. **Retrieve**: Backend searches ChromaDB for relevant context.
4. **Generate**: LLM answers the question using the retrieved context.

### B. The RLHF Training Workflow (Offline)

Once enough data is collected in the database, run the training scripts in order:

**1. Supervised Fine-Tuning (SFT)**
Teaches the model "how" to speak by mimicking good conversations.

```bash
python -m app.train_sft
```

**2. Reward Model Training**
Creates a digital judge that learns to distinguish between good (ğŸ‘) and bad (ğŸ‘) responses.

```bash
python -m app.train_reward
```

**3. Reinforcement Learning (PPO)**
Optimizes the policy model to maximize rewards generated by the Reward Model.

```bash
python -m app.train_rl
```

---
